#!/usr/bin/env python3
# Upload a list of files from the command-line to s3, possibly
# using a custom endpoint.
#
# This command is used as a baseline for testing s3 uploads
# via exodus-gw or via s3 directly. It can be used to compare
# performance and compatibility.
#
# Usage:
#
#  # Using default S3
#  examples/s3-upload file1 [file2 [...]]
#
#  # Testing same thing via exodus-gw
#  uvicorn exodus_gw.gateway:app &
#  examples/s3-upload --endpoint-url http://localhost:8000/upload file1 [file2 [...]]
#
# It is recommended to test using a mixture of files both smaller
# and larger than 10MB to cover both multipart and single part
# uploads.

import argparse
import hashlib
import logging
import os
import sys

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError


def get_object_key(filename):
    with open(filename, "rb") as f:
        hasher = hashlib.sha256()
        while True:
            chunk = f.read(1024 * 1024 * 10)
            if not chunk:
                break
            hasher.update(chunk)
        return hasher.hexdigest()


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--debug", action="store_true", help="Enable verbose logging"
    )
    parser.add_argument(
        "--endpoint-url", default="https://localhost:8010/upload"
    )
    parser.add_argument(
        "--cert",
        default=os.path.expandvars("${HOME}/certs/${USER}.crt"),
        help="Certificate for HTTPS authentication with exodus-gw (must match --key)",
    )
    parser.add_argument(
        "--key",
        default=os.path.expandvars("${HOME}/certs/${USER}.key"),
        help="Private key for HTTPS authentication with exodus-gw (must match --cert)",
    )
    parser.add_argument(
        "--meta",
        action="append",
        help="key=value pairs for additional metadata on uploaded object(s)",
    )

    parser.add_argument("--env", default="dev")
    parser.add_argument("files", nargs="+")

    args = parser.parse_args()

    if args.debug:
        logging.basicConfig(level=logging.DEBUG)

    s3 = boto3.resource(
        "s3",
        endpoint_url=args.endpoint_url,
        config=Config(client_cert=(args.cert, args.key)),
    )
    bucket = s3.Bucket(args.env)

    print(
        "Using endpoint:",
        "[default]" if not args.endpoint_url else args.endpoint_url,
    )

    metadata = {}
    for kv in args.meta or []:
        if "=" not in kv:
            # default if user didn't pass a value
            kv = kv + "=1"

        (k, v) = kv.split("=", 1)
        metadata[k] = v

    uploaded = {}
    for filename in args.files:
        uploaded[filename] = get_object_key(filename)

        # Check current state of object before upload.
        obj = bucket.Object(uploaded[filename])
        try:
            obj.load()
            print("Before upload:", filename, obj, obj.metadata)
        except ClientError as exc:
            if exc.response["Error"]["Code"] == "404":
                # Nothing wrong, it's just not uploaded yet.
                print("Not currently in bucket:", filename)
            else:
                # Anything else wrong, just re-raise.
                raise

        bucket.upload_file(
            filename,
            uploaded[filename],
            ExtraArgs=dict(Metadata=metadata),
        )
        print("Uploaded:", filename, uploaded[filename])

    # For everything we uploaded, try to get it back to verify that
    # the object really exists (and check the resulting metadata).
    for key in uploaded.values():
        obj = bucket.Object(key)
        obj.load()
        print("Verified:", obj, obj.metadata)


if __name__ == "__main__":
    main()
